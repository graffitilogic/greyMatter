September 1 2025

There is concern that the narrow focus, single-source training data.  I understand that a single source model with limited sampling allows us to test and measure capabilities for predictive outcomes but I'm also concerned about single-source-learning back-unfluencing the design of neural processing algorithm.

Seems like a good time to level set and clarify the plan.

I feel some level of design paralyis between cherry-picking Neo-Piaget concepts in structured information processing theory and this project's overall philosophy that is grounded more in a dynamic systems theory projection that true congition may emerge through complex interactions, not-necessarily as a function of scale, throughput and input parameters as is customary to traditional machine learning projects.

I'm reminded of the initial inspiration for this project: 

Games like No Man's Sky can apply limited classical compute procedural generation and perspective-distance-rendering without a full implementation of Bohmian Mechanics and still simulate a nearly infinite galaxy full of planets relative to player perspective. It seems to me that we should be able to model a biologically inspired neural learning cluster (yep: a brain) with similar, imperfect but still practical levels of fidelity by borrowing similar algorithms.

Just as the player's environment is rendered based on coordinates, time, a shared-seed generative function within a (deterministric, parameterized, resource-scalable) render distance of the player, so-too should a neural network be able to emulate massive scale within the render distance of a cognitive task.  

This is the intended end-state goal for this project.

A simulation and proof of concept that borrows concepts of short-lived, procedurally generated neural structures to overcome the traditonal limitations of neural-network scale. A focus less on input parameter volume and more on creating a highly complex (chaotic, even) dynamic biologically inspired machine learning system within the resource limits of classical computing. 

I understand in the same way that: A No Man's Sky rendering of a quadrant of space isn't the same detail as atomic-scale reality
that: our neural network will lack full fidelity of biological structures.   

We don't know what the limits and capabilities are without at least experimenting.

Desired end-state

A trained machine learning system that uses dynamically scaled neural structures in learning and cognition tasks.
When it learns.. the neural network will tap into procedure generation rules to hyrdate cortical columns necessary for specific tasks with minimal persistence as needed.
When it recognizes and responds... it should be able to regenerate those cortical columns as needed to answer a question, determine a result, within reason.

Eventually.. it shouldn't be dormant. It should be constantly running.... re-evaluating, consolidating, testing ideas.


Known Challenges:

"What" should be persisted versus procedurally generated?
possible mitigations:
(Should we continue to persist entire neural network relational structures or should we have secondary generative processes that establish procedural expansion rules based on weights, topics, etc)  - Are there designs in biology -or- machine learning -or- game development that provide algorithms that can be applied?

Scale - the human brain with its 86 billion neurons and countless interconnected complexity.
possible mitigation:  We don't really have to worry about the motor-sensory neurons, so instead of 86 billion - what is the number? Like 16 billion in the cerebral cortex for cognitive tasks and only a fraction of these are active for any given task.  Plus - they are slow.   Something like 10bits per second effective speed limit for human thought.    Meanwhile I'm sitting on multiple computers with 20-40 TFLOPS of CPU capacity and 100s of TFLOPS on the GPUs..  
I postulate that the massive datacenters being built today will eventually be considered overkill for human-equivalent AGI, if AGI even proves possible.  They could still drive the innovation necessary for the leap to aritificial superintelligence.. 






